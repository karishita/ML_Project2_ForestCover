{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karishita/ML_Project2_ForestCover/blob/main/CovType_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7nStPdCecTi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df=pd.read_csv('/content/covtype_clean.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "rubATzcifMKZ",
        "outputId": "731750c9-ceab-4bdf-c0e5-efc4ac9cc5a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
              "0       2596      51      3                               258   \n",
              "1       2590      56      2                               212   \n",
              "2       2804     139      9                               268   \n",
              "3       2785     155     18                               242   \n",
              "4       2595      45      2                               153   \n",
              "\n",
              "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
              "0                               0                              510   \n",
              "1                              -6                              390   \n",
              "2                              65                             3180   \n",
              "3                             118                             3090   \n",
              "4                              -1                              391   \n",
              "\n",
              "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
              "0            221             232            148   \n",
              "1            220             235            151   \n",
              "2            234             238            135   \n",
              "3            238             238            122   \n",
              "4            220             234            150   \n",
              "\n",
              "   Horizontal_Distance_To_Fire_Points  Cover_Type  soil_type  wilderness_area  \n",
              "0                                6279           5         29                1  \n",
              "1                                6225           5         29                1  \n",
              "2                                6121           2         12                1  \n",
              "3                                6211           2         30                1  \n",
              "4                                6172           5         29                1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24347596-356c-4cc2-85ec-712117e15607\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Elevation</th>\n",
              "      <th>Aspect</th>\n",
              "      <th>Slope</th>\n",
              "      <th>Horizontal_Distance_To_Hydrology</th>\n",
              "      <th>Vertical_Distance_To_Hydrology</th>\n",
              "      <th>Horizontal_Distance_To_Roadways</th>\n",
              "      <th>Hillshade_9am</th>\n",
              "      <th>Hillshade_Noon</th>\n",
              "      <th>Hillshade_3pm</th>\n",
              "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
              "      <th>Cover_Type</th>\n",
              "      <th>soil_type</th>\n",
              "      <th>wilderness_area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2596</td>\n",
              "      <td>51</td>\n",
              "      <td>3</td>\n",
              "      <td>258</td>\n",
              "      <td>0</td>\n",
              "      <td>510</td>\n",
              "      <td>221</td>\n",
              "      <td>232</td>\n",
              "      <td>148</td>\n",
              "      <td>6279</td>\n",
              "      <td>5</td>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2590</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>212</td>\n",
              "      <td>-6</td>\n",
              "      <td>390</td>\n",
              "      <td>220</td>\n",
              "      <td>235</td>\n",
              "      <td>151</td>\n",
              "      <td>6225</td>\n",
              "      <td>5</td>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2804</td>\n",
              "      <td>139</td>\n",
              "      <td>9</td>\n",
              "      <td>268</td>\n",
              "      <td>65</td>\n",
              "      <td>3180</td>\n",
              "      <td>234</td>\n",
              "      <td>238</td>\n",
              "      <td>135</td>\n",
              "      <td>6121</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2785</td>\n",
              "      <td>155</td>\n",
              "      <td>18</td>\n",
              "      <td>242</td>\n",
              "      <td>118</td>\n",
              "      <td>3090</td>\n",
              "      <td>238</td>\n",
              "      <td>238</td>\n",
              "      <td>122</td>\n",
              "      <td>6211</td>\n",
              "      <td>2</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2595</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>153</td>\n",
              "      <td>-1</td>\n",
              "      <td>391</td>\n",
              "      <td>220</td>\n",
              "      <td>234</td>\n",
              "      <td>150</td>\n",
              "      <td>6172</td>\n",
              "      <td>5</td>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24347596-356c-4cc2-85ec-712117e15607')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-24347596-356c-4cc2-85ec-712117e15607 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-24347596-356c-4cc2-85ec-712117e15607');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b6ab54a1-68d3-4c1d-9cb3-7f35a8db48fc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b6ab54a1-68d3-4c1d-9cb3-7f35a8db48fc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b6ab54a1-68d3-4c1d-9cb3-7f35a8db48fc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IoE4yyhfU2g",
        "outputId": "f46d0700-cd56-442d-e3dd-21a530f34cf1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(581012, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxYzirXLf779"
      },
      "outputs": [],
      "source": [
        "X=df.drop(columns=['Cover_Type'])\n",
        "y=df['Cover_Type']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols=['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points']"
      ],
      "metadata": {
        "id": "lRQnKkB6tLwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn__k7FjfXfk"
      },
      "outputs": [],
      "source": [
        "#Splitting into 70 % train data 30% test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train70_raw, X_test30_raw, y_train70, y_test30 = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhdKI30MhFPN"
      },
      "outputs": [],
      "source": [
        "#Validation Set 20% of training\n",
        "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(\n",
        "    X_train70_raw, y_train70, test_size=0.20, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbZlH4_1hXPW"
      },
      "outputs": [],
      "source": [
        "#Standard Scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_raw[numeric_cols] = scaler.fit_transform(X_train_raw[numeric_cols])\n",
        "X_valid_raw[numeric_cols] = scaler.transform(X_valid_raw[numeric_cols])\n",
        "X_test30_raw[numeric_cols] = scaler.transform(X_test30_raw[numeric_cols])\n",
        "X_train = X_train_raw\n",
        "X_valid = X_valid_raw\n",
        "X_test  = X_test30_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K39h36EhuNU",
        "outputId": "2c0346b5-abc8-4eb5-c379-3b9dc1e03088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp9JCeS8h2SQ",
        "outputId": "2af3f122-f5c8-465e-ce94-a024d78af3f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-17 12:11:17,715] A new study created in memory with name: no-name-51bcfeba-2333-4386-83d5-e2b838798d49\n",
            "[I 2025-11-17 12:11:20,639] Trial 0 finished with value: 0.879791497627302 and parameters: {'criterion': 'entropy', 'max_depth': 36, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.879791497627302.\n",
            "[I 2025-11-17 12:11:22,148] Trial 1 finished with value: 0.5907526247203166 and parameters: {'criterion': 'log_loss', 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 11, 'max_features': 'log2'}. Best is trial 0 with value: 0.879791497627302.\n",
            "[I 2025-11-17 12:11:33,236] Trial 2 finished with value: 0.8766934670895724 and parameters: {'criterion': 'log_loss', 'max_depth': 43, 'min_samples_split': 9, 'min_samples_leaf': 15, 'max_features': None}. Best is trial 0 with value: 0.879791497627302.\n",
            "[I 2025-11-17 12:11:35,940] Trial 3 finished with value: 0.8814265692999926 and parameters: {'criterion': 'log_loss', 'max_depth': 36, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.8814265692999926.\n",
            "[I 2025-11-17 12:11:36,433] Trial 4 finished with value: 0.4888372550465934 and parameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.8814265692999926.\n",
            "[I 2025-11-17 12:11:37,710] Trial 5 finished with value: 0.8464016129428832 and parameters: {'criterion': 'entropy', 'max_depth': 42, 'min_samples_split': 20, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 3 with value: 0.8814265692999926.\n",
            "[I 2025-11-17 12:11:39,429] Trial 6 finished with value: 0.8116962946571267 and parameters: {'criterion': 'entropy', 'max_depth': 40, 'min_samples_split': 20, 'min_samples_leaf': 18, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.8814265692999926.\n",
            "[I 2025-11-17 12:11:44,157] Trial 7 finished with value: 0.8581298714071451 and parameters: {'criterion': 'gini', 'max_depth': 41, 'min_samples_split': 11, 'min_samples_leaf': 18, 'max_features': None}. Best is trial 3 with value: 0.8814265692999926.\n",
            "[I 2025-11-17 12:11:45,357] Trial 8 finished with value: 0.827960954980207 and parameters: {'criterion': 'gini', 'max_depth': 37, 'min_samples_split': 6, 'min_samples_leaf': 12, 'max_features': 'log2'}. Best is trial 3 with value: 0.8814265692999926.\n",
            "[I 2025-11-17 12:11:47,926] Trial 9 finished with value: 0.7235376558235598 and parameters: {'criterion': 'gini', 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 3 with value: 0.8814265692999926.\n",
            "[I 2025-11-17 12:11:49,340] Trial 10 finished with value: 0.8578348208797423 and parameters: {'criterion': 'log_loss', 'max_depth': 25, 'min_samples_split': 15, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 3 with value: 0.8814265692999926.\n",
            "[I 2025-11-17 12:11:50,754] Trial 11 finished with value: 0.8894421086277692 and parameters: {'criterion': 'entropy', 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 11 with value: 0.8894421086277692.\n",
            "[I 2025-11-17 12:11:52,216] Trial 12 finished with value: 0.8902657913501021 and parameters: {'criterion': 'log_loss', 'max_depth': 28, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:11:54,310] Trial 13 finished with value: 0.8719972461950776 and parameters: {'criterion': 'entropy', 'max_depth': 24, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:11:55,633] Trial 14 finished with value: 0.8214575496053699 and parameters: {'criterion': 'log_loss', 'max_depth': 21, 'min_samples_split': 14, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:11:56,981] Trial 15 finished with value: 0.8537778761279535 and parameters: {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:11:58,250] Trial 16 finished with value: 0.8358535565882323 and parameters: {'criterion': 'log_loss', 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:11:59,505] Trial 17 finished with value: 0.8397506823043446 and parameters: {'criterion': 'entropy', 'max_depth': 49, 'min_samples_split': 14, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:11:59,741] Trial 18 finished with value: 0.06465294681714244 and parameters: {'criterion': 'log_loss', 'max_depth': 3, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:03,899] Trial 19 finished with value: 0.8720955963708785 and parameters: {'criterion': 'gini', 'max_depth': 31, 'min_samples_split': 17, 'min_samples_leaf': 13, 'max_features': None}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:05,397] Trial 20 finished with value: 0.8048609574389615 and parameters: {'criterion': 'log_loss', 'max_depth': 28, 'min_samples_split': 4, 'min_samples_leaf': 20, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:07,255] Trial 21 finished with value: 0.8803201298222321 and parameters: {'criterion': 'log_loss', 'max_depth': 33, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:08,428] Trial 22 finished with value: 0.7894322736101891 and parameters: {'criterion': 'log_loss', 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:09,872] Trial 23 finished with value: 0.8842541368542697 and parameters: {'criterion': 'log_loss', 'max_depth': 34, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:11,249] Trial 24 finished with value: 0.8828403530771312 and parameters: {'criterion': 'log_loss', 'max_depth': 26, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:12,576] Trial 25 finished with value: 0.8683951710063682 and parameters: {'criterion': 'entropy', 'max_depth': 33, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:13,917] Trial 26 finished with value: 0.8632563743207691 and parameters: {'criterion': 'log_loss', 'max_depth': 21, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:15,243] Trial 27 finished with value: 0.8680017703031644 and parameters: {'criterion': 'log_loss', 'max_depth': 50, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:20,203] Trial 28 finished with value: 0.8857908583511593 and parameters: {'criterion': 'gini', 'max_depth': 46, 'min_samples_split': 11, 'min_samples_leaf': 9, 'max_features': None}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:24,451] Trial 29 finished with value: 0.8857908583511593 and parameters: {'criterion': 'gini', 'max_depth': 46, 'min_samples_split': 11, 'min_samples_leaf': 9, 'max_features': None}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:28,561] Trial 30 finished with value: 0.8680386516190898 and parameters: {'criterion': 'gini', 'max_depth': 39, 'min_samples_split': 12, 'min_samples_leaf': 14, 'max_features': None}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:33,498] Trial 31 finished with value: 0.8857908583511593 and parameters: {'criterion': 'gini', 'max_depth': 46, 'min_samples_split': 11, 'min_samples_leaf': 9, 'max_features': None}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:37,672] Trial 32 finished with value: 0.8826805340414546 and parameters: {'criterion': 'gini', 'max_depth': 45, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': None}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:42,145] Trial 33 finished with value: 0.8781441321826363 and parameters: {'criterion': 'gini', 'max_depth': 47, 'min_samples_split': 13, 'min_samples_leaf': 11, 'max_features': None}. Best is trial 12 with value: 0.8902657913501021.\n",
            "[I 2025-11-17 12:12:46,785] Trial 34 finished with value: 0.8902903788940523 and parameters: {'criterion': 'gini', 'max_depth': 28, 'min_samples_split': 17, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 34 with value: 0.8902903788940523.\n",
            "[I 2025-11-17 12:12:52,034] Trial 35 finished with value: 0.8911632367042857 and parameters: {'criterion': 'gini', 'max_depth': 29, 'min_samples_split': 16, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 35 with value: 0.8911632367042857.\n",
            "[I 2025-11-17 12:12:56,920] Trial 36 finished with value: 0.9005679722652504 and parameters: {'criterion': 'entropy', 'max_depth': 29, 'min_samples_split': 18, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 36 with value: 0.9005679722652504.\n",
            "[I 2025-11-17 12:13:01,047] Trial 37 finished with value: 0.8762017162105677 and parameters: {'criterion': 'gini', 'max_depth': 22, 'min_samples_split': 18, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 36 with value: 0.9005679722652504.\n",
            "[I 2025-11-17 12:13:05,270] Trial 38 finished with value: 0.901367067443633 and parameters: {'criterion': 'entropy', 'max_depth': 28, 'min_samples_split': 17, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 38 with value: 0.901367067443633.\n",
            "[I 2025-11-17 12:13:09,840] Trial 39 finished with value: 0.8644488702023555 and parameters: {'criterion': 'entropy', 'max_depth': 18, 'min_samples_split': 17, 'min_samples_leaf': 7, 'max_features': None}. Best is trial 38 with value: 0.901367067443633.\n",
            "[I 2025-11-17 12:13:14,085] Trial 40 finished with value: 0.902190750165966 and parameters: {'criterion': 'entropy', 'max_depth': 36, 'min_samples_split': 19, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 40 with value: 0.902190750165966.\n",
            "[I 2025-11-17 12:13:18,550] Trial 41 finished with value: 0.9022276314818912 and parameters: {'criterion': 'entropy', 'max_depth': 37, 'min_samples_split': 19, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 41 with value: 0.9022276314818912.\n",
            "[I 2025-11-17 12:13:23,252] Trial 42 finished with value: 0.9022276314818912 and parameters: {'criterion': 'entropy', 'max_depth': 37, 'min_samples_split': 19, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 41 with value: 0.9022276314818912.\n",
            "[I 2025-11-17 12:13:27,532] Trial 43 finished with value: 0.9058420004425758 and parameters: {'criterion': 'entropy', 'max_depth': 37, 'min_samples_split': 19, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 43 with value: 0.9058420004425758.\n",
            "[I 2025-11-17 12:13:32,480] Trial 44 finished with value: 0.9039610533303828 and parameters: {'criterion': 'entropy', 'max_depth': 37, 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 43 with value: 0.9058420004425758.\n",
            "[I 2025-11-17 12:13:33,817] Trial 45 finished with value: 0.8538516387598043 and parameters: {'criterion': 'entropy', 'max_depth': 38, 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 43 with value: 0.9058420004425758.\n",
            "[I 2025-11-17 12:13:38,099] Trial 46 finished with value: 0.9057313564947997 and parameters: {'criterion': 'entropy', 'max_depth': 42, 'min_samples_split': 19, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 43 with value: 0.9058420004425758.\n",
            "[I 2025-11-17 12:13:42,473] Trial 47 finished with value: 0.9057313564947997 and parameters: {'criterion': 'entropy', 'max_depth': 42, 'min_samples_split': 19, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 43 with value: 0.9058420004425758.\n",
            "[I 2025-11-17 12:13:47,374] Trial 48 finished with value: 0.9057313564947997 and parameters: {'criterion': 'entropy', 'max_depth': 43, 'min_samples_split': 19, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 43 with value: 0.9058420004425758.\n",
            "[I 2025-11-17 12:13:51,678] Trial 49 finished with value: 0.9040962848221091 and parameters: {'criterion': 'entropy', 'max_depth': 43, 'min_samples_split': 20, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 43 with value: 0.9058420004425758.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Parameters: {'criterion': 'entropy', 'max_depth': 37, 'min_samples_split': 19, 'min_samples_leaf': 4, 'max_features': None}\n",
            "Best Validation Score: 0.9058420004425758\n",
            "\n",
            "Test Accuracy: 0.9153892050670094\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.91      0.92      0.91     63556\n",
            "           2       0.93      0.93      0.93     85078\n",
            "           3       0.90      0.92      0.91     10638\n",
            "           4       0.85      0.78      0.81       795\n",
            "           5       0.80      0.75      0.77      2941\n",
            "           6       0.85      0.83      0.84      5227\n",
            "           7       0.93      0.92      0.92      6069\n",
            "\n",
            "    accuracy                           0.92    174304\n",
            "   macro avg       0.88      0.86      0.87    174304\n",
            "weighted avg       0.92      0.92      0.92    174304\n",
            "\n",
            "Confusion Matrix\n",
            " [[58295  4801     6     0    65    11   378]\n",
            " [ 5258 78792   311     2   420   224    71]\n",
            " [    2   307  9758    77    41   453     0]\n",
            " [    0     1   144   618     0    32     0]\n",
            " [   70   594    50     0  2202    25     0]\n",
            " [   13   275   580    33    12  4314     0]\n",
            " [  416    74     0     0     2     0  5577]]\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix\n",
        "def objective(trial):\n",
        "\n",
        "    params = {\n",
        "        \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"]),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 50),\n",
        "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
        "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 20),\n",
        "        \"max_features\": trial.suggest_categorical(\"max_features\", [None, \"sqrt\", \"log2\"]),\n",
        "    }\n",
        "\n",
        "    # Force balanced class weights\n",
        "    model = DecisionTreeClassifier(\n",
        "        **params,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    preds = model.predict(X_valid)\n",
        "\n",
        "    return accuracy_score(y_valid, preds)\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print(\"\\nBest Parameters:\", study.best_params)\n",
        "print(\"Best Validation Score:\", study.best_value)\n",
        "\n",
        "# -----------------------------\n",
        "#  Training Best Model on the entire 56% train split\n",
        "# -----------------------------\n",
        "best_params = study.best_params\n",
        "best_model = DecisionTreeClassifier(**best_params, random_state=42)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# -----------------------------\n",
        "#  Final Evaluation on 30% Test Set\n",
        "# -----------------------------\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\nTest Accuracy:\", accuracy_score(y_test30, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test30, y_pred))\n",
        "print(\"Confusion Matrix\\n\",confusion_matrix(y_test30, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(best_model, \"decision_tree_optuna.pkl\")\n",
        "joblib.dump(scaler, \"scaler_dt.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvayBSxolV9P",
        "outputId": "0828f158-3bc5-4610-a76f-1676bd07d89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler_dt.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Random Forest\n",
        "import optuna\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 80, 200),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 20),\n",
        "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
        "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
        "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
        "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
        "        \"class_weight\": \"balanced\"\n",
        "    }\n",
        "\n",
        "    model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_valid)\n",
        "\n",
        "\n",
        "    return f1_score(y_valid, preds, average=\"macro\")\n",
        "\n",
        "\n",
        "# Run Optuna\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(\"Best parameters:\", study.best_params)\n",
        "print(\"Best macro F1 (validation):\", study.best_value)\n",
        "best_params = study.best_params\n",
        "best_params[\"class_weight\"] = \"balanced\"\n",
        "\n",
        "best_rf = RandomForestClassifier(\n",
        "    **best_params,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "best_rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test30, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test30, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNAfO7THmZpk",
        "outputId": "d469d0f3-6b4f-4e73-849d-184337253e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-17 12:17:15,091] A new study created in memory with name: no-name-2c325b4d-5b54-469e-b9f1-663143623b16\n",
            "[I 2025-11-17 12:17:46,971] Trial 0 finished with value: 0.5135201826471038 and parameters: {'n_estimators': 146, 'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 0 with value: 0.5135201826471038.\n",
            "[I 2025-11-17 12:21:49,766] Trial 1 finished with value: 0.6720761510319496 and parameters: {'n_estimators': 116, 'max_depth': 12, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': None, 'bootstrap': False}. Best is trial 1 with value: 0.6720761510319496.\n",
            "[I 2025-11-17 12:23:03,157] Trial 2 finished with value: 0.7197728297406538 and parameters: {'n_estimators': 122, 'max_depth': 12, 'min_samples_split': 19, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': False}. Best is trial 2 with value: 0.7197728297406538.\n",
            "[I 2025-11-17 12:24:42,549] Trial 3 finished with value: 0.753148411773024 and parameters: {'n_estimators': 161, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': 'log2', 'bootstrap': False}. Best is trial 3 with value: 0.753148411773024.\n",
            "[I 2025-11-17 12:27:53,863] Trial 4 finished with value: 0.694335961174546 and parameters: {'n_estimators': 147, 'max_depth': 11, 'min_samples_split': 9, 'min_samples_leaf': 7, 'max_features': None, 'bootstrap': True}. Best is trial 3 with value: 0.753148411773024.\n",
            "[I 2025-11-17 12:30:46,073] Trial 5 finished with value: 0.8830023631826108 and parameters: {'n_estimators': 183, 'max_depth': 20, 'min_samples_split': 18, 'min_samples_leaf': 2, 'max_features': 'log2', 'bootstrap': False}. Best is trial 5 with value: 0.8830023631826108.\n",
            "[I 2025-11-17 12:33:52,720] Trial 6 finished with value: 0.8180586567423781 and parameters: {'n_estimators': 100, 'max_depth': 16, 'min_samples_split': 7, 'min_samples_leaf': 7, 'max_features': None, 'bootstrap': True}. Best is trial 5 with value: 0.8830023631826108.\n",
            "[I 2025-11-17 12:39:18,387] Trial 7 finished with value: 0.8231106820585382 and parameters: {'n_estimators': 178, 'max_depth': 16, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': None, 'bootstrap': True}. Best is trial 5 with value: 0.8830023631826108.\n",
            "[I 2025-11-17 12:42:52,885] Trial 8 finished with value: 0.8226012063341134 and parameters: {'n_estimators': 117, 'max_depth': 16, 'min_samples_split': 14, 'min_samples_leaf': 3, 'max_features': None, 'bootstrap': True}. Best is trial 5 with value: 0.8830023631826108.\n",
            "[I 2025-11-17 12:44:05,289] Trial 9 finished with value: 0.8243579966294193 and parameters: {'n_estimators': 134, 'max_depth': 17, 'min_samples_split': 12, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 5 with value: 0.8830023631826108.\n",
            "[I 2025-11-17 12:47:08,274] Trial 10 finished with value: 0.8825307963912143 and parameters: {'n_estimators': 199, 'max_depth': 20, 'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': False}. Best is trial 5 with value: 0.8830023631826108.\n",
            "[I 2025-11-17 12:50:02,147] Trial 11 finished with value: 0.88180834027447 and parameters: {'n_estimators': 193, 'max_depth': 20, 'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': False}. Best is trial 5 with value: 0.8830023631826108.\n",
            "[I 2025-11-17 12:52:55,228] Trial 12 finished with value: 0.8869824670485675 and parameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 16, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': False}. Best is trial 12 with value: 0.8869824670485675.\n",
            "[I 2025-11-17 12:55:23,812] Trial 13 finished with value: 0.8748423517477487 and parameters: {'n_estimators': 175, 'max_depth': 19, 'min_samples_split': 16, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': False}. Best is trial 12 with value: 0.8869824670485675.\n",
            "[I 2025-11-17 12:56:24,303] Trial 14 finished with value: 0.5485154360158656 and parameters: {'n_estimators': 180, 'max_depth': 7, 'min_samples_split': 16, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': False}. Best is trial 12 with value: 0.8869824670485675.\n",
            "[I 2025-11-17 12:58:38,188] Trial 15 finished with value: 0.8530161314686102 and parameters: {'n_estimators': 162, 'max_depth': 18, 'min_samples_split': 17, 'min_samples_leaf': 7, 'max_features': 'log2', 'bootstrap': False}. Best is trial 12 with value: 0.8869824670485675.\n",
            "[I 2025-11-17 13:00:52,010] Trial 16 finished with value: 0.7858294288089915 and parameters: {'n_estimators': 200, 'max_depth': 14, 'min_samples_split': 14, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'bootstrap': False}. Best is trial 12 with value: 0.8869824670485675.\n",
            "[I 2025-11-17 13:01:29,442] Trial 17 finished with value: 0.6209882672538082 and parameters: {'n_estimators': 87, 'max_depth': 9, 'min_samples_split': 18, 'min_samples_leaf': 2, 'max_features': 'log2', 'bootstrap': False}. Best is trial 12 with value: 0.8869824670485675.\n",
            "[I 2025-11-17 13:04:02,119] Trial 18 finished with value: 0.8464299456621258 and parameters: {'n_estimators': 186, 'max_depth': 18, 'min_samples_split': 14, 'min_samples_leaf': 10, 'max_features': 'log2', 'bootstrap': False}. Best is trial 12 with value: 0.8869824670485675.\n",
            "[I 2025-11-17 13:05:55,679] Trial 19 finished with value: 0.7798524434135449 and parameters: {'n_estimators': 164, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 6, 'max_features': 'log2', 'bootstrap': False}. Best is trial 12 with value: 0.8869824670485675.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 16, 'min_samples_leaf': 3, 'max_features': 'log2', 'bootstrap': False}\n",
            "Best macro F1 (validation): 0.8869824670485675\n",
            "Test Accuracy: 0.9293418395447035\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.94      0.93      0.93     63556\n",
            "           2       0.95      0.92      0.94     85078\n",
            "           3       0.91      0.93      0.92     10638\n",
            "           4       0.78      0.92      0.84       795\n",
            "           5       0.66      0.93      0.77      2941\n",
            "           6       0.79      0.94      0.86      5227\n",
            "           7       0.90      0.98      0.94      6069\n",
            "\n",
            "    accuracy                           0.93    174304\n",
            "   macro avg       0.85      0.94      0.89    174304\n",
            "weighted avg       0.93      0.93      0.93    174304\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(best_rf, \"rf_optuna.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNEET28x8AJD",
        "outputId": "e85db0a8-88ec-4476-cb98-1b195cd9a3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rf_optuna.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression\n"
      ],
      "metadata": {
        "id": "R9RPNZRMVijQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    # Hyperparameters to tune\n",
        "    C = trial.suggest_float(\"C\", 1e-3, 1e2, log=True)\n",
        "    solver = trial.suggest_categorical(\"solver\", [\"lbfgs\", \"saga\"])\n",
        "\n",
        "    # Handle valid (solver, penalty, multi_class) combinations\n",
        "    #if solver == \"liblinear\":\n",
        "        #penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])\n",
        "        #multi_class = \"ovr\"\n",
        "    if solver == \"lbfgs\":\n",
        "        penalty = \"l2\"              # lbfgs supports only l2 here\n",
        "        multi_class = \"multinomial\"\n",
        "    else:  # \"saga\"\n",
        "        penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])\n",
        "        multi_class = \"multinomial\"\n",
        "\n",
        "    model = LogisticRegression(\n",
        "        C=C,\n",
        "        solver=solver,\n",
        "        penalty=penalty,\n",
        "        multi_class=multi_class,\n",
        "        class_weight=\"balanced\",\n",
        "        max_iter=500,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train on 56% train split\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Validate on 14% validation split\n",
        "    preds = model.predict(X_valid)\n",
        "\n",
        "    return accuracy_score(y_valid, preds)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "#  Run Optuna Study\n",
        "# -----------------------------\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"\\nBest Parameters:\", study.best_params)\n",
        "print(\"Best Validation Score:\", study.best_value)\n",
        "\n",
        "# -----------------------------\n",
        "#  Training Best Model on the entire 56% train split\n",
        "#\n",
        "# -----------------------------\n",
        "best_params = study.best_params\n",
        "\n",
        "C_best = best_params[\"C\"]\n",
        "solver_best = best_params[\"solver\"]\n",
        "penalty_best = best_params.get(\"penalty\", \"l2\")\n",
        "\n",
        "#if solver_best == \"liblinear\":\n",
        "    #multi_class_best = \"ovr\"\n",
        "#else:\n",
        "multi_class_best = \"multinomial\"\n",
        "\n",
        "best_model = LogisticRegression(\n",
        "    C=C_best,\n",
        "    solver=solver_best,\n",
        "    penalty=penalty_best,\n",
        "    multi_class=multi_class_best,\n",
        "    class_weight=\"balanced\",\n",
        "    max_iter=500,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# -----------------------------\n",
        "#  Final Evaluation on 30% Test Set\n",
        "#  (X_test must be the scaled version of X_test30_raw)\n",
        "# -----------------------------\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\nTest Accuracy:\", accuracy_score(y_test30, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test30, y_pred))\n",
        "print(\"Confusion Matrix\\n\", confusion_matrix(y_test30, y_pred))\n"
      ],
      "metadata": {
        "id": "aEPCTDKJVn9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ead8f69-b196-4507-d1a9-8ae513d5dc48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-08 15:05:50,340] A new study created in memory with name: no-name-a230cdba-2dc8-42fd-a98a-ed064b56cd55\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "[I 2025-12-08 15:07:56,658] Trial 0 finished with value: 0.5555186742396302 and parameters: {'C': 52.56136453273163, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.5555186742396302.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "[I 2025-12-08 15:10:02,459] Trial 1 finished with value: 0.5556047306434561 and parameters: {'C': 0.20340166009014893, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.5556047306434561.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-12-08 15:11:49,561] Trial 2 finished with value: 0.5519657741388213 and parameters: {'C': 0.01431950219358555, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.5556047306434561.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-12-08 15:13:21,928] Trial 3 finished with value: 0.5499127142189767 and parameters: {'C': 23.561445513978956, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.5556047306434561.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "[I 2025-12-08 15:14:24,185] Trial 4 finished with value: 0.5461139386786654 and parameters: {'C': 0.0047822572244299606, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.5556047306434561.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-12-08 15:20:10,211] Trial 5 finished with value: 0.5493717882520714 and parameters: {'C': 3.0953965513047583, 'solver': 'saga', 'penalty': 'l1'}. Best is trial 1 with value: 0.5556047306434561.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-12-08 15:23:42,455] Trial 6 finished with value: 0.567640333407096 and parameters: {'C': 1.824150341095121, 'solver': 'saga', 'penalty': 'l2'}. Best is trial 6 with value: 0.567640333407096.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-12-08 15:26:50,333] Trial 7 finished with value: 0.5423643382262546 and parameters: {'C': 0.20678264237130956, 'solver': 'saga', 'penalty': 'l1'}. Best is trial 6 with value: 0.567640333407096.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "[I 2025-12-08 15:27:31,909] Trial 8 finished with value: 0.5513756730840156 and parameters: {'C': 4.709722675108857, 'solver': 'lbfgs'}. Best is trial 6 with value: 0.567640333407096.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2025-12-08 15:29:51,874] Trial 9 finished with value: 0.5442329915664724 and parameters: {'C': 0.8176254543450063, 'solver': 'saga', 'penalty': 'l2'}. Best is trial 6 with value: 0.567640333407096.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Parameters: {'C': 1.824150341095121, 'solver': 'saga', 'penalty': 'l2'}\n",
            "Best Validation Score: 0.567640333407096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.5681797319625482\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.67      0.60      0.63     63556\n",
            "           2       0.77      0.52      0.62     85078\n",
            "           3       0.69      0.54      0.60     10638\n",
            "           4       0.23      0.88      0.37       795\n",
            "           5       0.10      0.73      0.17      2941\n",
            "           6       0.34      0.57      0.43      5227\n",
            "           7       0.31      0.90      0.46      6069\n",
            "\n",
            "    accuracy                           0.57    174304\n",
            "   macro avg       0.44      0.67      0.47    174304\n",
            "weighted avg       0.68      0.57      0.60    174304\n",
            "\n",
            "Confusion Matrix\n",
            " [[37986 12325    11    11  2311   262 10650]\n",
            " [18291 44069  1328    72 16987  2640  1691]\n",
            " [    0    46  5725  1572   552  2743     0]\n",
            " [    0     0    69   697     0    29     0]\n",
            " [    4   651    82     0  2140    64     0]\n",
            " [    0   124  1127   647   345  2984     0]\n",
            " [  587    24     0     0    23     0  5435]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "zXG6yIkzmFbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import optuna\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.utils import resample\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    # -----------------------------\n",
        "    #   SUBSAMPLING\n",
        "    # -----------------------------\n",
        "    SUBSAMPLE_SIZE = 5000\n",
        "\n",
        "    X_sub, y_sub = resample(\n",
        "        X_train, y_train,\n",
        "        n_samples=SUBSAMPLE_SIZE,\n",
        "        stratify=y_train,\n",
        "        random_state=42 + trial.number\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    #  Common hyperparameters\n",
        "    # -----------------------------\n",
        "    C = trial.suggest_float(\"C\", 1e-2, 1e2, log=True)\n",
        "    gamma = trial.suggest_float(\"gamma\", 1e-4, 1e-1, log=True)\n",
        "\n",
        "\n",
        "    model = SVC(\n",
        "        kernel=\"rbf\",\n",
        "        C=C,\n",
        "        gamma=gamma,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    #  Train on SUBSAMPLED data\n",
        "    # -----------------------------\n",
        "    model.fit(X_sub, y_sub)\n",
        "\n",
        "    # -----------------------------\n",
        "    #  Validate on FULL validation set\n",
        "    # -----------------------------\n",
        "    preds = model.predict(X_valid)\n",
        "\n",
        "    return accuracy_score(y_valid, preds)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "#  Run Optuna\n",
        "# -----------------------------\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"\\nBest Parameters:\", study.best_params)\n",
        "print(\"Best Validation Score:\", study.best_value)\n",
        "\n",
        "\n",
        "best_params = study.best_params\n",
        "\n",
        "best_model = SVC(\n",
        "    kernel=\"rbf\",\n",
        "    C=best_params[\"C\"],\n",
        "    gamma=best_params[\"gamma\"],\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_final, y_final = resample(\n",
        "    X_train, y_train,\n",
        "    n_samples=50000,\n",
        "    stratify=y_train,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "best_model.fit(X_final, y_final)\n",
        "\n",
        "# ---- evaluate on full test set ----\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\nTest Accuracy:\", accuracy_score(y_test30, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test30, y_pred))\n",
        "print(\"Confusion Matrix\\n\", confusion_matrix(y_test30, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7i6UsCGmAwK",
        "outputId": "d21a7a34-8088-4b3d-8df8-8d04f8f27aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-10 04:27:00,780] A new study created in memory with name: no-name-6b122329-6588-4db5-99ad-008b1b0d0032\n",
            "[I 2025-12-10 04:27:28,937] Trial 0 finished with value: 0.6326129182955914 and parameters: {'C': 59.24262431296422, 'gamma': 0.004774390763237564}. Best is trial 0 with value: 0.6326129182955914.\n",
            "[I 2025-12-10 04:27:53,762] Trial 1 finished with value: 0.3347962921983723 and parameters: {'C': 0.03587348388094603, 'gamma': 0.0003648882648492446}. Best is trial 0 with value: 0.6326129182955914.\n",
            "[I 2025-12-10 04:28:14,857] Trial 2 finished with value: 0.5714882840353077 and parameters: {'C': 8.085382184394007, 'gamma': 0.0007469778115061107}. Best is trial 0 with value: 0.6326129182955914.\n",
            "[I 2025-12-10 04:28:38,662] Trial 3 finished with value: 0.37843918271003907 and parameters: {'C': 1.7204670819626897, 'gamma': 0.00015826302101423073}. Best is trial 0 with value: 0.6326129182955914.\n",
            "[I 2025-12-10 04:29:02,417] Trial 4 finished with value: 0.42377861375427206 and parameters: {'C': 0.3072791166460146, 'gamma': 0.002786519544928756}. Best is trial 0 with value: 0.6326129182955914.\n",
            "[I 2025-12-10 04:29:18,624] Trial 5 finished with value: 0.69004942096334 and parameters: {'C': 23.78983610452667, 'gamma': 0.026393209796686398}. Best is trial 5 with value: 0.69004942096334.\n",
            "[I 2025-12-10 04:29:38,066] Trial 6 finished with value: 0.5857982346143443 and parameters: {'C': 66.99607168476855, 'gamma': 0.0009656307695387655}. Best is trial 5 with value: 0.69004942096334.\n",
            "[I 2025-12-10 04:30:09,314] Trial 7 finished with value: 0.46410218583265717 and parameters: {'C': 0.0425881002908085, 'gamma': 0.018260643982019695}. Best is trial 5 with value: 0.69004942096334.\n",
            "[I 2025-12-10 04:30:33,908] Trial 8 finished with value: 0.31222492685205677 and parameters: {'C': 0.09420825220782336, 'gamma': 0.00010958355883520967}. Best is trial 5 with value: 0.69004942096334.\n",
            "[I 2025-12-10 04:30:58,057] Trial 9 finished with value: 0.40043274077352414 and parameters: {'C': 0.02215659100856742, 'gamma': 0.004173086524218261}. Best is trial 5 with value: 0.69004942096334.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Parameters: {'C': 23.78983610452667, 'gamma': 0.026393209796686398}\n",
            "Best Validation Score: 0.69004942096334\n",
            "\n",
            "Test Accuracy: 0.7274818707545437\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.75      0.76      0.75     63556\n",
            "           2       0.85      0.68      0.75     85078\n",
            "           3       0.76      0.74      0.75     10638\n",
            "           4       0.54      0.86      0.67       795\n",
            "           5       0.23      0.88      0.37      2941\n",
            "           6       0.46      0.81      0.59      5227\n",
            "           7       0.56      0.96      0.71      6069\n",
            "\n",
            "    accuracy                           0.73    174304\n",
            "   macro avg       0.59      0.81      0.65    174304\n",
            "weighted avg       0.77      0.73      0.74    174304\n",
            "\n",
            "Confusion Matrix\n",
            " [[48040  9933    45     0  1383   119  4036]\n",
            " [15720 57605  1589    11  7053  2528   572]\n",
            " [    0    81  7846   440   146  2125     0]\n",
            " [    0     0    65   685     0    45     0]\n",
            " [   35   191    74     0  2589    49     3]\n",
            " [    4    63   769   122    51  4218     0]\n",
            " [  242     6     0     0     1     0  5820]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network\n",
        "\n",
        "Without class weights"
      ],
      "metadata": {
        "id": "NVbmVrG5s_pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "#  Imports\n",
        "# ============================\n",
        "import numpy as np\n",
        "import optuna\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# ============================\n",
        "#  Prepare NN-specific labels\n",
        "#  (1..7  ->  0..6)\n",
        "# ============================\n",
        "print(\"Original train labels:\", np.unique(y_train))\n",
        "\n",
        "y_train_nn = y_train - 1\n",
        "y_valid_nn = y_valid - 1\n",
        "y_test30_nn = y_test30 - 1\n",
        "\n",
        "print(\"NN train labels after shift:\", np.unique(y_train_nn))\n",
        "\n",
        "# Features / classes info\n",
        "input_dim = X_train.shape[1]\n",
        "n_classes = len(np.unique(y_train_nn))\n",
        "\n",
        "\n",
        "# ============================\n",
        "#  Build model function\n",
        "# ============================\n",
        "def create_model(trial):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Hyperparameters to tune\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
        "    units = trial.suggest_int(\"units\", 64, 256, step=64)\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
        "\n",
        "    # Input + first hidden layer\n",
        "    model.add(Dense(units, activation=\"relu\", input_shape=(input_dim,)))\n",
        "    if dropout_rate > 0:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Additional hidden layers\n",
        "    for _ in range(n_layers - 1):\n",
        "        model.add(Dense(units, activation=\"relu\"))\n",
        "        if dropout_rate > 0:\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(n_classes, activation=\"softmax\"))\n",
        "\n",
        "    # Compile with sparse categorical crossentropy\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# ============================\n",
        "#  Optuna objective function\n",
        "#\n",
        "# ============================\n",
        "def objective(trial):\n",
        "    # Clear previous graph between trials\n",
        "    K.clear_session()\n",
        "\n",
        "    model = create_model(trial)\n",
        "\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
        "    epochs = trial.suggest_int(\"epochs\", 10, 40)\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor=\"val_accuracy\",\n",
        "        patience=3,\n",
        "        restore_best_weights=True,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train_nn,\n",
        "        validation_data=(X_valid, y_valid_nn),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Best validation accuracy during training\n",
        "    val_acc = max(history.history[\"val_accuracy\"])\n",
        "    return val_acc\n",
        "\n",
        "\n",
        "# ============================\n",
        "#  Run Optuna Study\n",
        "# ============================\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10)   # change 10 to 5/20 if you like\n",
        "\n",
        "print(\"\\nBest Parameters:\", study.best_params)\n",
        "print(\"Best Validation Accuracy:\", study.best_value)\n",
        "\n",
        "\n",
        "# ============================\n",
        "#  Build best model\n",
        "#\n",
        "# ============================\n",
        "best_params = study.best_params\n",
        "K.clear_session()\n",
        "\n",
        "def build_best_model(best_params):\n",
        "    model = Sequential()\n",
        "\n",
        "    n_layers = best_params[\"n_layers\"]\n",
        "    units = best_params[\"units\"]\n",
        "    dropout_rate = best_params[\"dropout_rate\"]\n",
        "    learning_rate = best_params[\"learning_rate\"]\n",
        "\n",
        "    model.add(Dense(units, activation=\"relu\", input_shape=(input_dim,)))\n",
        "    if dropout_rate > 0:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    for _ in range(n_layers - 1):\n",
        "        model.add(Dense(units, activation=\"relu\"))\n",
        "        if dropout_rate > 0:\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(n_classes, activation=\"softmax\"))\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "best_model = build_best_model(best_params)\n",
        "\n",
        "early_stopping_final = EarlyStopping(\n",
        "    monitor=\"val_accuracy\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "history_final = best_model.fit(\n",
        "    X_train, y_train_nn,\n",
        "    validation_split=0.1,\n",
        "    epochs=best_params[\"epochs\"],\n",
        "    batch_size=best_params[\"batch_size\"],\n",
        "    callbacks=[early_stopping_final],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# ============================\n",
        "#  Final Evaluation on Test Set\n",
        "# ============================\n",
        "y_pred_proba = best_model.predict(X_test)\n",
        "y_pred_nn = np.argmax(y_pred_proba, axis=1)      # predicted labels in 0..6\n",
        "\n",
        "print(\"\\nTest Accuracy (NN, sparse):\", accuracy_score(y_test30_nn, y_pred_nn))\n",
        "print(\"\\nClassification Report (NN):\\n\",\n",
        "      classification_report(y_test30_nn, y_pred_nn))\n",
        "print(\"Confusion Matrix (NN):\\n\",\n",
        "      confusion_matrix(y_test30_nn, y_pred_nn))\n",
        "\n",
        "\n",
        "# predictions back in original 1..7 label space:\n",
        "y_pred_original = y_pred_nn + 1\n",
        "print(\"\\nFirst 20 predicted labels in original space (1..7):\", y_pred_original[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CldB5AsmmcMa",
        "outputId": "dc8edbc2-dda8-45a4-e2fa-3c63bbfcadb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-10 04:52:28,483] A new study created in memory with name: no-name-11955265-f5ee-4d0c-be16-13d99daf79a9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original train labels: [1 2 3 4 5 6 7]\n",
            "NN train labels after shift: [0 1 2 3 4 5 6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 04:55:03,779] Trial 0 finished with value: 0.8447050452232361 and parameters: {'n_layers': 3, 'units': 256, 'dropout_rate': 0.3701593716995501, 'learning_rate': 0.0011497301252093093, 'batch_size': 128, 'epochs': 38}. Best is trial 0 with value: 0.8447050452232361.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 04:56:40,707] Trial 1 finished with value: 0.8227115273475647 and parameters: {'n_layers': 3, 'units': 256, 'dropout_rate': 0.2747603161886326, 'learning_rate': 0.004915834773623337, 'batch_size': 256, 'epochs': 25}. Best is trial 0 with value: 0.8447050452232361.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 04:57:33,284] Trial 2 finished with value: 0.8609205484390259 and parameters: {'n_layers': 3, 'units': 256, 'dropout_rate': 0.03516237373626119, 'learning_rate': 0.005989623615841982, 'batch_size': 256, 'epochs': 13}. Best is trial 2 with value: 0.8609205484390259.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 04:58:35,805] Trial 3 finished with value: 0.707764744758606 and parameters: {'n_layers': 3, 'units': 256, 'dropout_rate': 0.42170306890083764, 'learning_rate': 0.005532681632217041, 'batch_size': 64, 'epochs': 30}. Best is trial 2 with value: 0.8609205484390259.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 04:59:54,928] Trial 4 finished with value: 0.7607139945030212 and parameters: {'n_layers': 1, 'units': 192, 'dropout_rate': 0.4685966301670146, 'learning_rate': 0.000458740218230127, 'batch_size': 256, 'epochs': 25}. Best is trial 2 with value: 0.8609205484390259.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:01:55,003] Trial 5 finished with value: 0.7603329420089722 and parameters: {'n_layers': 3, 'units': 64, 'dropout_rate': 0.3102588430552439, 'learning_rate': 0.003160548016928023, 'batch_size': 128, 'epochs': 38}. Best is trial 2 with value: 0.8609205484390259.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:04:56,633] Trial 6 finished with value: 0.732438325881958 and parameters: {'n_layers': 2, 'units': 192, 'dropout_rate': 0.46464481291841797, 'learning_rate': 0.004699499102728977, 'batch_size': 64, 'epochs': 16}. Best is trial 2 with value: 0.8609205484390259.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:07:34,505] Trial 7 finished with value: 0.8268422484397888 and parameters: {'n_layers': 2, 'units': 64, 'dropout_rate': 0.044824590811945286, 'learning_rate': 0.0008043277479994628, 'batch_size': 128, 'epochs': 26}. Best is trial 2 with value: 0.8609205484390259.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:13:39,275] Trial 8 finished with value: 0.7964397072792053 and parameters: {'n_layers': 2, 'units': 128, 'dropout_rate': 0.31114274015799864, 'learning_rate': 0.0001283706855942764, 'batch_size': 64, 'epochs': 26}. Best is trial 2 with value: 0.8609205484390259.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:17:23,382] Trial 9 finished with value: 0.7412161231040955 and parameters: {'n_layers': 2, 'units': 64, 'dropout_rate': 0.4344085764053358, 'learning_rate': 0.00011067633622211621, 'batch_size': 128, 'epochs': 32}. Best is trial 2 with value: 0.8609205484390259.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Parameters: {'n_layers': 3, 'units': 256, 'dropout_rate': 0.03516237373626119, 'learning_rate': 0.005989623615841982, 'batch_size': 256, 'epochs': 13}\n",
            "Best Validation Accuracy: 0.8609205484390259\n",
            "Epoch 1/13\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.6894 - loss: 0.7705 - val_accuracy: 0.7523 - val_loss: 0.5665\n",
            "Epoch 2/13\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7633 - loss: 0.5482 - val_accuracy: 0.7841 - val_loss: 0.4981\n",
            "Epoch 3/13\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7876 - loss: 0.4971 - val_accuracy: 0.8039 - val_loss: 0.4592\n",
            "Epoch 4/13\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8009 - loss: 0.4656 - val_accuracy: 0.8204 - val_loss: 0.4223\n",
            "Epoch 5/13\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8135 - loss: 0.4401 - val_accuracy: 0.8307 - val_loss: 0.4031\n",
            "Epoch 6/13\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8198 - loss: 0.4277 - val_accuracy: 0.8323 - val_loss: 0.3970\n",
            "Epoch 7/13\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8269 - loss: 0.4094 - val_accuracy: 0.8342 - val_loss: 0.3897\n",
            "Epoch 8/13\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8264 - loss: 0.4103 - val_accuracy: 0.8411 - val_loss: 0.3737\n",
            "Epoch 9/13\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8350 - loss: 0.3954 - val_accuracy: 0.8489 - val_loss: 0.3618\n",
            "Epoch 10/13\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8362 - loss: 0.3906 - val_accuracy: 0.8510 - val_loss: 0.3579\n",
            "Epoch 11/13\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8402 - loss: 0.3813 - val_accuracy: 0.8499 - val_loss: 0.3586\n",
            "Epoch 12/13\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8436 - loss: 0.3779 - val_accuracy: 0.8540 - val_loss: 0.3448\n",
            "Epoch 13/13\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8421 - loss: 0.3756 - val_accuracy: 0.8487 - val_loss: 0.3530\n",
            "Restoring model weights from the end of the best epoch: 12.\n",
            "\u001b[1m5447/5447\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step\n",
            "\n",
            "Test Accuracy (NN, sparse): 0.8590221681659629\n",
            "\n",
            "Classification Report (NN):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.84      0.85     63556\n",
            "           1       0.86      0.91      0.88     85078\n",
            "           2       0.85      0.87      0.86     10638\n",
            "           3       0.85      0.62      0.72       795\n",
            "           4       0.87      0.35      0.50      2941\n",
            "           5       0.71      0.76      0.73      5227\n",
            "           6       0.85      0.80      0.83      6069\n",
            "\n",
            "    accuracy                           0.86    174304\n",
            "   macro avg       0.84      0.73      0.77    174304\n",
            "weighted avg       0.86      0.86      0.86    174304\n",
            "\n",
            "Confusion Matrix (NN):\n",
            " [[53101  9737     3     0    22     7   686]\n",
            " [ 6729 77072   500     0   120   512   145]\n",
            " [    1   332  9231    70     2  1002     0]\n",
            " [    0     0   218   494     0    83     0]\n",
            " [   44  1760    93     0  1023    21     0]\n",
            " [   36   345   867    19     6  3954     0]\n",
            " [ 1146    67     0     0     0     0  4856]]\n",
            "\n",
            "First 20 predicted labels in original space (1..7): [1 2 2 2 2 3 2 1 2 2 2 1 5 2 2 2 1 5 3 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Class weights to handle imbalance\n"
      ],
      "metadata": {
        "id": "R5BaP89bzpmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "#  Imports\n",
        "# ============================\n",
        "import numpy as np\n",
        "import optuna\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# ============================\n",
        "#  Prepare NN-specific labels\n",
        "#  (1..7  ->  0..6)\n",
        "# ============================\n",
        "print(\"Original train labels:\", np.unique(y_train))\n",
        "\n",
        "y_train_nn = y_train - 1\n",
        "y_valid_nn = y_valid - 1\n",
        "y_test30_nn = y_test30 - 1\n",
        "\n",
        "print(\"NN train labels after shift:\", np.unique(y_train_nn))\n",
        "\n",
        "# Features / classes info\n",
        "input_dim = X_train.shape[1]\n",
        "n_classes = len(np.unique(y_train_nn))\n",
        "classes = np.unique(y_train_nn)\n",
        "class_weights_array = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=classes,\n",
        "    y=y_train_nn\n",
        ")\n",
        "class_weight_dict = {cls: w for cls, w in zip(classes, class_weights_array)}\n",
        "print(class_weight_dict)\n",
        "\n",
        "\n",
        "# ============================\n",
        "#  Build model function\n",
        "# ============================\n",
        "def create_model(trial):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Hyperparameters to tune\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
        "    units = trial.suggest_int(\"units\", 64, 256, step=64)\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
        "\n",
        "    # Input + first hidden layer\n",
        "    model.add(Dense(units, activation=\"relu\", input_shape=(input_dim,)))\n",
        "    if dropout_rate > 0:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Additional hidden layers\n",
        "    for _ in range(n_layers - 1):\n",
        "        model.add(Dense(units, activation=\"relu\"))\n",
        "        if dropout_rate > 0:\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(n_classes, activation=\"softmax\"))\n",
        "\n",
        "    # Compile with sparse categorical crossentropy\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# ============================\n",
        "#  Optuna objective function\n",
        "#  (Train on X_train, validate on X_valid)\n",
        "# ============================\n",
        "def objective(trial):\n",
        "    # Clear previous graph between trials\n",
        "    K.clear_session()\n",
        "\n",
        "    model = create_model(trial)\n",
        "\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
        "    epochs = trial.suggest_int(\"epochs\", 10, 40)\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor=\"val_accuracy\",\n",
        "        patience=3,\n",
        "        restore_best_weights=True,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train_nn,\n",
        "        validation_data=(X_valid, y_valid_nn),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=0,\n",
        "        class_weight=class_weight_dict\n",
        "    )\n",
        "\n",
        "    # Best validation accuracy during training\n",
        "    val_acc = max(history.history[\"val_accuracy\"])\n",
        "    return val_acc\n",
        "\n",
        "\n",
        "# ============================\n",
        "#  Run Optuna Study\n",
        "# ============================\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10)   # change 10 to 5/20 if you like\n",
        "\n",
        "print(\"\\nBest Parameters:\", study.best_params)\n",
        "print(\"Best Validation Accuracy:\", study.best_value)\n",
        "\n",
        "\n",
        "# ============================\n",
        "#  Build best model (same arch as best trial)\n",
        "#  Train ONLY on X_train (no train+valid mixing)\n",
        "# ============================\n",
        "best_params = study.best_params\n",
        "K.clear_session()\n",
        "\n",
        "def build_best_model(best_params):\n",
        "    model = Sequential()\n",
        "\n",
        "    n_layers = best_params[\"n_layers\"]\n",
        "    units = best_params[\"units\"]\n",
        "    dropout_rate = best_params[\"dropout_rate\"]\n",
        "    learning_rate = best_params[\"learning_rate\"]\n",
        "\n",
        "    model.add(Dense(units, activation=\"relu\", input_shape=(input_dim,)))\n",
        "    if dropout_rate > 0:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    for _ in range(n_layers - 1):\n",
        "        model.add(Dense(units, activation=\"relu\"))\n",
        "        if dropout_rate > 0:\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(n_classes, activation=\"softmax\"))\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "best_model = build_best_model(best_params)\n",
        "\n",
        "early_stopping_final = EarlyStopping(\n",
        "    monitor=\"val_accuracy\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train final model ONLY on X_train (as you requested)\n",
        "history_final = best_model.fit(\n",
        "    X_train, y_train_nn,\n",
        "    validation_split=0.1,                    # small internal split from X_train itself\n",
        "    epochs=best_params[\"epochs\"],\n",
        "    batch_size=best_params[\"batch_size\"],\n",
        "    callbacks=[early_stopping_final],\n",
        "    verbose=1,\n",
        "    class_weight=class_weight_dict\n",
        ")\n",
        "\n",
        "\n",
        "# ============================\n",
        "#  Final Evaluation on Test Set\n",
        "# ============================\n",
        "y_pred_proba = best_model.predict(X_test)\n",
        "y_pred_nn = np.argmax(y_pred_proba, axis=1)      # predicted labels in 0..6\n",
        "\n",
        "print(\"\\nTest Accuracy (NN, sparse):\", accuracy_score(y_test30_nn, y_pred_nn))\n",
        "print(\"\\nClassification Report (NN):\\n\",\n",
        "      classification_report(y_test30_nn, y_pred_nn))\n",
        "print(\"Confusion Matrix (NN):\\n\",\n",
        "      confusion_matrix(y_test30_nn, y_pred_nn))\n",
        "\n",
        "\n",
        "# If you want predictions back in original 1..7 label space:\n",
        "y_pred_original = y_pred_nn + 1\n",
        "print(\"\\nFirst 20 predicted labels in original space (1..7):\", y_pred_original[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYPLhqFxuCrd",
        "outputId": "d59a3dfb-ba87-4c9d-dd8a-7e0b1ac23cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-10 05:29:43,057] A new study created in memory with name: no-name-b6822447-2c45-43ee-8f88-af72e7f42842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original train labels: [1 2 3 4 5 6 7]\n",
            "NN train labels after shift: [0 1 2 3 4 5 6]\n",
            "{np.int64(0): np.float64(0.39243891171855305), np.int64(1): np.float64(0.2930974376066913), np.int64(2): np.float64(2.3096078083407274), np.int64(3): np.float64(30.045802936559237), np.int64(4): np.float64(8.7222475404123), np.int64(5): np.float64(4.802237539297153), np.int64(6): np.float64(3.986351384464592)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:31:17,078] Trial 0 finished with value: 0.6333628296852112 and parameters: {'n_layers': 3, 'units': 64, 'dropout_rate': 0.18001858272537063, 'learning_rate': 0.002460611706988979, 'batch_size': 128, 'epochs': 28}. Best is trial 0 with value: 0.6333628296852112.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:36:05,162] Trial 1 finished with value: 0.7027366161346436 and parameters: {'n_layers': 3, 'units': 192, 'dropout_rate': 0.1430761953345947, 'learning_rate': 0.00011536818943556975, 'batch_size': 64, 'epochs': 23}. Best is trial 1 with value: 0.7027366161346436.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:37:16,425] Trial 2 finished with value: 0.6334120035171509 and parameters: {'n_layers': 1, 'units': 256, 'dropout_rate': 0.39446920548411335, 'learning_rate': 0.000314513874791665, 'batch_size': 128, 'epochs': 10}. Best is trial 1 with value: 0.7027366161346436.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:38:40,894] Trial 3 finished with value: 0.6625482439994812 and parameters: {'n_layers': 1, 'units': 192, 'dropout_rate': 0.013739754209528221, 'learning_rate': 0.0006056945442801341, 'batch_size': 128, 'epochs': 17}. Best is trial 1 with value: 0.7027366161346436.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:40:44,382] Trial 4 finished with value: 0.6897789835929871 and parameters: {'n_layers': 3, 'units': 256, 'dropout_rate': 0.34131319244080527, 'learning_rate': 0.000568722454911795, 'batch_size': 128, 'epochs': 27}. Best is trial 1 with value: 0.7027366161346436.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:42:15,452] Trial 5 finished with value: 0.610877513885498 and parameters: {'n_layers': 3, 'units': 192, 'dropout_rate': 0.2575194984892898, 'learning_rate': 0.0041989063649227635, 'batch_size': 64, 'epochs': 10}. Best is trial 1 with value: 0.7027366161346436.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:43:10,011] Trial 6 finished with value: 0.7453222274780273 and parameters: {'n_layers': 2, 'units': 192, 'dropout_rate': 0.10025835650610726, 'learning_rate': 0.0014278451030678193, 'batch_size': 256, 'epochs': 14}. Best is trial 6 with value: 0.7453222274780273.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:44:50,784] Trial 7 finished with value: 0.531140148639679 and parameters: {'n_layers': 3, 'units': 64, 'dropout_rate': 0.38516654101359404, 'learning_rate': 0.00010951193915745955, 'batch_size': 256, 'epochs': 36}. Best is trial 6 with value: 0.7453222274780273.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:45:47,544] Trial 8 finished with value: 0.5375083088874817 and parameters: {'n_layers': 3, 'units': 64, 'dropout_rate': 0.4777696400150168, 'learning_rate': 0.0044875033965621515, 'batch_size': 128, 'epochs': 28}. Best is trial 6 with value: 0.7453222274780273.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "[I 2025-12-10 05:47:24,188] Trial 9 finished with value: 0.6188192963600159 and parameters: {'n_layers': 1, 'units': 128, 'dropout_rate': 0.18900839282911025, 'learning_rate': 0.006954287934165614, 'batch_size': 64, 'epochs': 27}. Best is trial 6 with value: 0.7453222274780273.\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Parameters: {'n_layers': 2, 'units': 192, 'dropout_rate': 0.10025835650610726, 'learning_rate': 0.0014278451030678193, 'batch_size': 256, 'epochs': 14}\n",
            "Best Validation Accuracy: 0.7453222274780273\n",
            "Epoch 1/14\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.4081 - loss: 1.0751 - val_accuracy: 0.6045 - val_loss: 0.8835\n",
            "Epoch 2/14\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5856 - loss: 0.6852 - val_accuracy: 0.6342 - val_loss: 0.8105\n",
            "Epoch 3/14\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6188 - loss: 0.6113 - val_accuracy: 0.6553 - val_loss: 0.7452\n",
            "Epoch 4/14\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6367 - loss: 0.5715 - val_accuracy: 0.6576 - val_loss: 0.7443\n",
            "Epoch 5/14\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.6555 - loss: 0.5359 - val_accuracy: 0.6834 - val_loss: 0.7105\n",
            "Epoch 6/14\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6681 - loss: 0.5067 - val_accuracy: 0.6586 - val_loss: 0.7739\n",
            "Epoch 7/14\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6779 - loss: 0.4863 - val_accuracy: 0.7026 - val_loss: 0.6768\n",
            "Epoch 8/14\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.6864 - loss: 0.4700 - val_accuracy: 0.7213 - val_loss: 0.6189\n",
            "Epoch 9/14\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6951 - loss: 0.4554 - val_accuracy: 0.7057 - val_loss: 0.6697\n",
            "Epoch 10/14\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7036 - loss: 0.4387 - val_accuracy: 0.6960 - val_loss: 0.6947\n",
            "Epoch 11/14\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7080 - loss: 0.4304 - val_accuracy: 0.7152 - val_loss: 0.6520\n",
            "Epoch 12/14\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7169 - loss: 0.4158 - val_accuracy: 0.7378 - val_loss: 0.5909\n",
            "Epoch 13/14\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7126 - loss: 0.4196 - val_accuracy: 0.7525 - val_loss: 0.5665\n",
            "Epoch 14/14\n",
            "\u001b[1m1144/1144\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7221 - loss: 0.4046 - val_accuracy: 0.7343 - val_loss: 0.6098\n",
            "Restoring model weights from the end of the best epoch: 13.\n",
            "\u001b[1m5447/5447\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step\n",
            "\n",
            "Test Accuracy (NN, sparse): 0.7547216357628053\n",
            "\n",
            "Classification Report (NN):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.77      0.77     63556\n",
            "           1       0.86      0.71      0.78     85078\n",
            "           2       0.82      0.80      0.81     10638\n",
            "           3       0.52      0.95      0.68       795\n",
            "           4       0.26      0.93      0.40      2941\n",
            "           5       0.54      0.88      0.67      5227\n",
            "           6       0.59      0.96      0.73      6069\n",
            "\n",
            "    accuracy                           0.75    174304\n",
            "   macro avg       0.62      0.86      0.69    174304\n",
            "weighted avg       0.79      0.75      0.76    174304\n",
            "\n",
            "Confusion Matrix (NN):\n",
            " [[48901  9810    11     0  1218    96  3520]\n",
            " [14144 60297  1380    32  6423  2249   553]\n",
            " [    1    33  8463   509   153  1479     0]\n",
            " [    0     0    21   756     0    18     0]\n",
            " [    8   140    29     0  2730    34     0]\n",
            " [    0    28   379   145    89  4586     0]\n",
            " [  246     4     0     0     1     0  5818]]\n",
            "\n",
            "First 20 predicted labels in original space (1..7): [1 5 6 2 5 3 2 1 2 2 2 1 5 2 2 2 1 5 3 3]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyN4o5AyqY/lwpoEuKzJUgW9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}